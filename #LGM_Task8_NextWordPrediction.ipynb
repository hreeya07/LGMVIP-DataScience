{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbd51c",
   "metadata": {},
   "source": [
    "# HREEYA SINGH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4b185",
   "metadata": {},
   "source": [
    "# LGMVIP - DataScience Intern, March-2022\n",
    "## Task8 - Next Word Prediction\n",
    "### Advanced Level Task\n",
    "Using Tensorflow and Keras library train a RNN, to predict the next word.\n",
    "\n",
    "Dataset Link: https://drive.google.com/file/d/1GeUzNVqiixXHnTl8oNiQ2W3CynX_lsu2/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d18d82",
   "metadata": {},
   "source": [
    "Import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7addff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import heapq\n",
    "import warnings as wg\n",
    "wg.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f631ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source text\n",
    "file = open('1661-0.txt', encoding=\"utf8\").read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a546709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 581888\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14faf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"1661-0.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f377e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net   Title: The Adventures of Sherl\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216c4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Project Gutenberg s The Adventures of Sherlock Holmes  by Arthur Conan Doyle  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever   You may copy it  give it away or re use it under the terms of the Project Gutenberg License included with this eBook or online at www gutenberg net   Title  The Adventures of Sherlock Holmes  Author  Arthur Conan Doyle  Release Date  November 29  2002  EBook  1661  Last Updated  May 20  2019  Language  English  Charact'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccca9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use anyone anywhere at no cost and with almost restrictions whatsoever. You may copy it, give it away or re-use under terms Gutenberg License included this online www.gutenberg.net Title: Holmes Author: Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set encoding: UTF-8 *** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES SHERLOCK HOLMES Prod\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97c1a7",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8017ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[838, 3083, 56, 322, 57, 1523, 15, 95, 839, 3084]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ebf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d58bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  17678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 838, 3083],\n",
       "       [3083,   56],\n",
       "       [  56,  322],\n",
       "       [ 322,   57],\n",
       "       [  57, 1523],\n",
       "       [1523,   15],\n",
       "       [  15,   95],\n",
       "       [  95,  839],\n",
       "       [ 839, 3084],\n",
       "       [3084, 3085]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197fea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 838 3083   56  322   57]\n",
      "The responses are:  [3083   56  322   57 1523]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f230b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e62cb",
   "metadata": {},
   "source": [
    "## RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be39a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ab0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             89310     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8931)              8939931   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,078,241\n",
      "Trainable params: 22,078,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40f83f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## call back\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97eda942",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d4f9fc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 8.9427\n",
      "Epoch 00001: loss improved from inf to 8.94268, saving model to nextword1.h5\n",
      "118/118 [==============================] - 42s 326ms/step - loss: 8.9427 - lr: 0.0010\n",
      "Epoch 2/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 8.4303\n",
      "Epoch 00002: loss improved from 8.94268 to 8.43034, saving model to nextword1.h5\n",
      "118/118 [==============================] - 41s 351ms/step - loss: 8.4303 - lr: 0.0010\n",
      "Epoch 3/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 8.1412\n",
      "Epoch 00003: loss improved from 8.43034 to 8.14122, saving model to nextword1.h5\n",
      "118/118 [==============================] - 39s 330ms/step - loss: 8.1412 - lr: 0.0010\n",
      "Epoch 4/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 8.0014\n",
      "Epoch 00004: loss improved from 8.14122 to 8.00145, saving model to nextword1.h5\n",
      "118/118 [==============================] - 34s 284ms/step - loss: 8.0014 - lr: 0.0010\n",
      "Epoch 5/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.8859\n",
      "Epoch 00005: loss improved from 8.00145 to 7.88591, saving model to nextword1.h5\n",
      "118/118 [==============================] - 35s 301ms/step - loss: 7.8859 - lr: 0.0010\n",
      "Epoch 6/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.7795\n",
      "Epoch 00006: loss improved from 7.88591 to 7.77951, saving model to nextword1.h5\n",
      "118/118 [==============================] - 33s 278ms/step - loss: 7.7795 - lr: 0.0010\n",
      "Epoch 7/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.6401\n",
      "Epoch 00007: loss improved from 7.77951 to 7.64009, saving model to nextword1.h5\n",
      "118/118 [==============================] - 30s 252ms/step - loss: 7.6401 - lr: 0.0010\n",
      "Epoch 8/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.4470\n",
      "Epoch 00008: loss improved from 7.64009 to 7.44704, saving model to nextword1.h5\n",
      "118/118 [==============================] - 30s 252ms/step - loss: 7.4470 - lr: 0.0010\n",
      "Epoch 9/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.2307\n",
      "Epoch 00009: loss improved from 7.44704 to 7.23073, saving model to nextword1.h5\n",
      "118/118 [==============================] - 30s 253ms/step - loss: 7.2307 - lr: 0.0010\n",
      "Epoch 10/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 7.0295\n",
      "Epoch 00010: loss improved from 7.23073 to 7.02954, saving model to nextword1.h5\n",
      "118/118 [==============================] - 35s 296ms/step - loss: 7.0295 - lr: 0.0010\n",
      "Epoch 11/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.8464\n",
      "Epoch 00011: loss improved from 7.02954 to 6.84638, saving model to nextword1.h5\n",
      "118/118 [==============================] - 40s 341ms/step - loss: 6.8464 - lr: 0.0010\n",
      "Epoch 12/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.6814\n",
      "Epoch 00012: loss improved from 6.84638 to 6.68139, saving model to nextword1.h5\n",
      "118/118 [==============================] - 39s 333ms/step - loss: 6.6814 - lr: 0.0010\n",
      "Epoch 13/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.5377\n",
      "Epoch 00013: loss improved from 6.68139 to 6.53769, saving model to nextword1.h5\n",
      "118/118 [==============================] - 41s 350ms/step - loss: 6.5377 - lr: 0.0010\n",
      "Epoch 14/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.4058\n",
      "Epoch 00014: loss improved from 6.53769 to 6.40584, saving model to nextword1.h5\n",
      "118/118 [==============================] - 35s 296ms/step - loss: 6.4058 - lr: 0.0010\n",
      "Epoch 15/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.2870\n",
      "Epoch 00015: loss improved from 6.40584 to 6.28697, saving model to nextword1.h5\n",
      "118/118 [==============================] - 39s 330ms/step - loss: 6.2870 - lr: 0.0010\n",
      "Epoch 16/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.1815\n",
      "Epoch 00016: loss improved from 6.28697 to 6.18154, saving model to nextword1.h5\n",
      "118/118 [==============================] - 36s 304ms/step - loss: 6.1815 - lr: 0.0010\n",
      "Epoch 17/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 6.0801\n",
      "Epoch 00017: loss improved from 6.18154 to 6.08008, saving model to nextword1.h5\n",
      "118/118 [==============================] - 35s 300ms/step - loss: 6.0801 - lr: 0.0010\n",
      "Epoch 18/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.9971\n",
      "Epoch 00018: loss improved from 6.08008 to 5.99713, saving model to nextword1.h5\n",
      "118/118 [==============================] - 33s 283ms/step - loss: 5.9971 - lr: 0.0010\n",
      "Epoch 19/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.9107\n",
      "Epoch 00019: loss improved from 5.99713 to 5.91072, saving model to nextword1.h5\n",
      "118/118 [==============================] - 36s 308ms/step - loss: 5.9107 - lr: 0.0010\n",
      "Epoch 20/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.8340\n",
      "Epoch 00020: loss improved from 5.91072 to 5.83401, saving model to nextword1.h5\n",
      "118/118 [==============================] - 49s 415ms/step - loss: 5.8340 - lr: 0.0010\n",
      "Epoch 21/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.7582\n",
      "Epoch 00021: loss improved from 5.83401 to 5.75823, saving model to nextword1.h5\n",
      "118/118 [==============================] - 48s 408ms/step - loss: 5.7582 - lr: 0.0010\n",
      "Epoch 22/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.6808\n",
      "Epoch 00022: loss improved from 5.75823 to 5.68079, saving model to nextword1.h5\n",
      "118/118 [==============================] - 38s 319ms/step - loss: 5.6808 - lr: 0.0010\n",
      "Epoch 23/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.6132\n",
      "Epoch 00023: loss improved from 5.68079 to 5.61323, saving model to nextword1.h5\n",
      "118/118 [==============================] - 33s 278ms/step - loss: 5.6132 - lr: 0.0010\n",
      "Epoch 24/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.5451\n",
      "Epoch 00024: loss improved from 5.61323 to 5.54514, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 275ms/step - loss: 5.5451 - lr: 0.0010\n",
      "Epoch 25/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.4734\n",
      "Epoch 00025: loss improved from 5.54514 to 5.47338, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 271ms/step - loss: 5.4734 - lr: 0.0010\n",
      "Epoch 26/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.4092\n",
      "Epoch 00026: loss improved from 5.47338 to 5.40920, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 267ms/step - loss: 5.4092 - lr: 0.0010\n",
      "Epoch 27/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.3502\n",
      "Epoch 00027: loss improved from 5.40920 to 5.35022, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 268ms/step - loss: 5.3502 - lr: 0.0010\n",
      "Epoch 28/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.2775\n",
      "Epoch 00028: loss improved from 5.35022 to 5.27754, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 268ms/step - loss: 5.2775 - lr: 0.0010\n",
      "Epoch 29/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.2083\n",
      "Epoch 00029: loss improved from 5.27754 to 5.20833, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 273ms/step - loss: 5.2083 - lr: 0.0010\n",
      "Epoch 30/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.1356\n",
      "Epoch 00030: loss improved from 5.20833 to 5.13565, saving model to nextword1.h5\n",
      "118/118 [==============================] - 33s 282ms/step - loss: 5.1356 - lr: 0.0010\n",
      "Epoch 31/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 5.0483\n",
      "Epoch 00031: loss improved from 5.13565 to 5.04831, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 275ms/step - loss: 5.0483 - lr: 0.0010\n",
      "Epoch 32/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.9573\n",
      "Epoch 00032: loss improved from 5.04831 to 4.95734, saving model to nextword1.h5\n",
      "118/118 [==============================] - 31s 265ms/step - loss: 4.9573 - lr: 0.0010\n",
      "Epoch 33/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.8420\n",
      "Epoch 00033: loss improved from 4.95734 to 4.84203, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 267ms/step - loss: 4.8420 - lr: 0.0010\n",
      "Epoch 34/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.7450\n",
      "Epoch 00034: loss improved from 4.84203 to 4.74504, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 267ms/step - loss: 4.7450 - lr: 0.0010\n",
      "Epoch 35/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.6478\n",
      "Epoch 00035: loss improved from 4.74504 to 4.64777, saving model to nextword1.h5\n",
      "118/118 [==============================] - 31s 263ms/step - loss: 4.6478 - lr: 0.0010\n",
      "Epoch 36/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.5385\n",
      "Epoch 00036: loss improved from 4.64777 to 4.53853, saving model to nextword1.h5\n",
      "118/118 [==============================] - 31s 266ms/step - loss: 4.5385 - lr: 0.0010\n",
      "Epoch 37/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.4595\n",
      "Epoch 00037: loss improved from 4.53853 to 4.45954, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 268ms/step - loss: 4.4595 - lr: 0.0010\n",
      "Epoch 38/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.3783\n",
      "Epoch 00038: loss improved from 4.45954 to 4.37828, saving model to nextword1.h5\n",
      "118/118 [==============================] - 33s 282ms/step - loss: 4.3783 - lr: 0.0010\n",
      "Epoch 39/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.3008\n",
      "Epoch 00039: loss improved from 4.37828 to 4.30084, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 269ms/step - loss: 4.3008 - lr: 0.0010\n",
      "Epoch 40/40\n",
      "118/118 [==============================] - ETA: 0s - loss: 4.2208\n",
      "Epoch 00040: loss improved from 4.30084 to 4.22080, saving model to nextword1.h5\n",
      "118/118 [==============================] - 32s 270ms/step - loss: 4.2208 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137b495f1c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c743bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: My name is Hreeya Singh\n",
      "['is', 'Hreeya', 'Singh']\n",
      "”\n",
      "Enter your line: wash your hand\n",
      "['wash', 'your', 'hand']\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 3).\n",
      "describe\n",
      "Enter your line: wash them nice and clean\n",
      "['nice', 'and', 'clean']\n",
      "joke\n",
      "Enter your line: scrub them here\n",
      "['scrub', 'them', 'here']\n",
      "”\n",
      "Enter your line: play our handy game\n",
      "['our', 'handy', 'game']\n",
      "fad\n",
      "Enter your line: \n",
      "  @media print {\n",
      "    .ms-editor-squiggles-container {\n",
      "      display:none !important;\n",
      "    }\n",
      "  }\n",
      "  .ms-editor-squiggles-container {\n",
      "    all: initial;\n",
      "  }germs go down the drain\n",
      "['down', 'the', 'drain']\n",
      "highest\n",
      "Enter your line: wash your hands\n",
      "['wash', 'your', 'hands']\n",
      "”\n",
      "Enter your line: Thank you\n",
      "['Thank', 'you']\n",
      "”\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224595af",
   "metadata": {},
   "source": [
    "## Thank you:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff2e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
